{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeI4Swht7HvmP7iN6L11iC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kghdxx/8.2CDevSecOps/blob/main/Task2Part1_Kaylin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F, types as T\n",
        "from pyspark.sql.functions import from_unixtime, to_date, col\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "from pyspark.sql import DataFrame\n"
      ],
      "metadata": {
        "id": "i-9qQXEXsQCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00QXoiUAxP7W"
      },
      "outputs": [],
      "source": [
        "#Q1 (boilerplate from Darrin's code)\n",
        "\n",
        "# Download the data for part 1 from the the GitHub repository\n",
        "!wget -q https://github.com/tulip-lab/sit742/raw/develop/Jupyter/data/business_review_submission.zip\n",
        "\n",
        "# Unzip the folder int othe data directory\n",
        "!unzip business_review_submission.zip -d data\n",
        "\n",
        "# Remove the zip file\n",
        "!rm business_review_submission.zip\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SIT742-A2-P1\").getOrCreate()\n",
        "\n",
        "# Load data\n",
        "data_file_path = \"./data/review.csv\"\n",
        "sdf = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Inspect headers\n",
        "sdf.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.1.1\n",
        "For the none or null in text column, change it to 'no review'"
      ],
      "metadata": {
        "id": "1tS_dpah3zWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf_text_without_null = sdf.na.fill({'text': 'no review'})"
      ],
      "metadata": {
        "id": "AIdKuXyr3xWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.1.2 Process the content in time column, and convert the strings from time to yyyy-mm-dd format\n",
        "in the new column as newtime and show the first 5 rows.\n"
      ],
      "metadata": {
        "id": "43_YeaeggCOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add new date value, 'newtime_long' as dummy column\n",
        "sdf_with_new_date = (\n",
        "    sdf_text_without_null\n",
        "    .withColumn(\"newtime_long\", col(\"time\").cast(\"long\"))\n",
        "    .withColumn(\"newtime\", to_date(from_unixtime( (col(\"newtime_long\")/1000).cast(\"long\") )))\n",
        ")\n",
        "\n",
        "sdf_with_new_date.show(20)\n",
        "\n",
        "\n",
        "'''\n",
        "This changes the unix datetime value to long type so that it can be converted\n",
        "to a shorter version of unix time value, so that it can finally be converted\n",
        "properly with spark's \"to_date\" function to YYYY-mm-dd\n",
        "'''"
      ],
      "metadata": {
        "id": "vDO-2-9YsOvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.2\n",
        "\n",
        "Find out the information for gmap_id on the reviews. In order to achieve the above, some wrangling\n",
        "work is required to be done:\n"
      ],
      "metadata": {
        "id": "X65XQIErxZEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F, types as T\n",
        "\n",
        "# Typically, incoming data should have schema defined on read,\n",
        "# The line of questioning asks for wrangling in this step (Q1.2) so performing\n",
        "# a second read with defined schema.\n",
        "\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"user_id\",     T.FloatType(),  True),\n",
        "    T.StructField(\"name\",        T.StringType(),  True),\n",
        "    T.StructField(\"time\",       T.LongType(), True),\n",
        "    T.StructField(\"rating\",       T.IntegerType(), True),\n",
        "    T.StructField(\"text\",        T.StringType(),  True),\n",
        "    T.StructField(\"pics\",       T.StringType(), True),\n",
        "    T.StructField(\"resp\",       T.StringType(), True),\n",
        "    T.StructField(\"gmap_id\",     T.StringType(),  True),\n",
        "])\n",
        "\n",
        "required = [\"user_id\"] # enforce non-null after read\n",
        "enforce_types_on = [\"time\",\"rating\" ] # drop rows where columns failed to parse\n",
        "\n",
        "sdf = (spark.read\n",
        "       .schema(schema)\n",
        "       .option(\"header\", True)\n",
        "       .csv(\"./data/review.csv\"))\n",
        "\n",
        "# Keep rows where key is present AND typed columns parsed (not null)\n",
        "sdf_clean = sdf.dropna(subset=required + enforce_types_on)\n",
        "\n",
        "# Clean-up the name column for incorrect and unwanted names\n",
        "\n",
        "# Tidy up whitespacing\n",
        "sdf_clean_name = sdf_clean.withColumn(\"name\", F.regexp_replace(F.trim(\"name\"), r\"\\s+\", \" \"))\n",
        "\n",
        "# Accept only letters, spaces, dots and sizes between 2 and 60 for name\n",
        "name_regex = r\"^(?=.{2,60}$)\\p{L}+(?:[ .'-]\\p{L}+)*$\"\n",
        "\n",
        "# Filter out incorrect charachters in name column\n",
        "sdf_clean_name = sdf_clean_name.filter(F.col(\"name\").rlike(name_regex))\n",
        "\n",
        "# View and inspect\n",
        "inspect_cleaned_data = sdf_clean_name.describe()\n",
        "\n",
        "inspect_cleaned_data.show(truncate = False)\n",
        "\n",
        "sdf_clean_name.show(5)\n",
        "\n",
        "# re-add the newtime column\n",
        "\n",
        "sdf_cleaned_with_new_dates = (\n",
        "    sdf_clean_name\n",
        "    .withColumn(\"newtime_long\", col(\"time\").cast(\"long\"))\n",
        "    .withColumn(\"newtime\", to_date(from_unixtime( (col(\"newtime_long\")/1000).cast(\"long\") )))\n",
        ")\n",
        "\n",
        "# Drop helper colum, not needed\n",
        "sdf_cleaned_with_date = sdf_cleaned_with_new_dates.drop(\"newtime_long\")\n",
        "\n",
        "# Inspect again\n",
        "sdf_cleaned_with_date.show(5)"
      ],
      "metadata": {
        "id": "p0f1UYE-jpLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.2.1 Using pyspark to calculate the number of reviews per each unique gmap_id and save as float\n",
        "format in pyspark dataframe to show the top 5 rows.\n"
      ],
      "metadata": {
        "id": "NCB-RDnRxc8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use agg to define the float as part of the call\n",
        "gmap_gby = (\n",
        "    sdf_cleaned_with_date\n",
        "      .filter(F.col(\"gmap_id\").isNotNull())\n",
        "      .groupBy(\"gmap_id\")\n",
        "      .agg(F.count(\"*\").cast(\"float\").alias(\"review_count\"))\n",
        "      .orderBy(F.desc(\"review_count\"))\n",
        ")\n",
        "gmap_gby.show(5, truncate=False)\n",
        "\n",
        "'''\n",
        "Create a new temporay sdf without nulls (for the purpose of this question only) then take a groupby of all the gmap_id values to ensure a unique set, add a count aggregation of each row of for each matching gmap_id.\n",
        "Cast this column as a float.\n",
        "Sort descending, from highest to lowest.\n",
        "'''\n"
      ],
      "metadata": {
        "id": "T2Li7iFD1NzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.2.2 Transform the current pyspark dataframe to pandas dataframe (named as df)\n",
        "and create the column reivew_time with the information of review time on hours\n",
        "level. Print your df pandas dataframe with top 5 rows after creating the column\n",
        "review_time."
      ],
      "metadata": {
        "id": "Y61mtDKdN7wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conevert the Spark df to pandas\n",
        "\n",
        "sdf_valid_dates = sdf_cleaned_with_date.na.drop(subset=[\"newtime\"])\n",
        "\n",
        "df = sdf_valid_dates.toPandas()\n",
        "\n",
        "# Create a Date time column\n",
        "\n",
        "df[\"date_time\"] = pd.to_datetime(df[\"time\"].astype(int), unit=\"ms\")\n",
        "\n",
        "# time in hours\n",
        "df[\"review_time\"] = df[\"date_time\"].dt.hour\n",
        "\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "id": "TlCaqZK_MTuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.2.3\n",
        "\n",
        "Using matplotlib or seaborn to draw some (two or more if possible) visualizations on the\n",
        "relationship between gmap_id and reivew_time. You could explore for example, what is the\n",
        "time people usually review? How many business is reviewed in the morning time etc. Please\n",
        "also discuss the insights you are finding with your visualizations in the markdown cell. Please\n",
        "also include your findings and visualizations in the report."
      ],
      "metadata": {
        "id": "SG8Ag3nFaxIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use existing code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "counts = (df['review_time']\n",
        "          .value_counts()\n",
        "          #.reindex(range(24), fill_value=0)\n",
        "          .sort_index())\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "counts.plot(kind='bar')\n",
        "plt.title('Reviews by Hour of Day')\n",
        "plt.xlabel('Hour (0–23)')\n",
        "plt.ylabel('Review Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zeCwfd0OS-Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.3 Let’s continue to analyze the reivew_time with reviews and related gmap_id. You need to use\n",
        "another data meta-business to join with the current dataframe on gmap_id.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aZGpC1V5bLkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.3.1  Determine which workday (day of the week), generates the most reviews (plotting the results\n",
        "in a line chart with workday on averaged submissions)."
      ],
      "metadata": {
        "id": "bvX98WvcbYsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 1.3.1 Determine which workday (day of the week), generates the most reviews (plotting the results\n",
        "in a line chart with workday on averaged submissions).\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "# Use the time column to get a DateTime object\n",
        "# extract the day of the week from the DateTime object\n",
        "df[\"workday\"] = df[\"date_time\"].dt.strftime(\"%A\")\n",
        "\n",
        "# Also need the week number to average the submissions\n",
        "df[\"week\"] = df[\"date_time\"].dt.isocalendar().week\n",
        "\n",
        "# Calculate the number of reviews for each day in each week\n",
        "weekly_counts = df.groupby([\"week\", \"workday\"]).size().reset_index(name=\"review_counts\")\n",
        "\n",
        "# Calculate average reviews per weekday across all weeks\n",
        "# Reindex for plotting\n",
        "average_reviews = weekly_counts.groupby(\"workday\")['review_counts'].mean().reindex([\n",
        "    \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"\n",
        "])\n",
        "\n",
        "# Identify the weekday with the highest average reviews\n",
        "busiest_workday = average_reviews.idxmax()\n",
        "print(f\"Workday with the highest average number of reviews: {busiest_workday}\")\n",
        "\n",
        "# Plotting the results in a line chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(average_reviews.index, average_reviews.values, marker='o', linestyle='-')\n",
        "plt.title(\"Average Submissions per Workday\")\n",
        "plt.xlabel(\"Workday\")\n",
        "plt.ylabel(\"Average Number of Reviews\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "51L92AVUbe85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.3.2"
      ],
      "metadata": {
        "id": "S1g9BzZ_p-fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 1.3.2 Identify the names of business (column name from data meta-business) that has the highest\n",
        "averaged ratings on ‘that workday’ (you need to find out from 1.3.1), and find out which\n",
        "category those businesses are from?\n",
        "\"\"\"\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "# Load meta-review-business data\n",
        "meta_file_path = \"./data/meta-review-business.csv\"\n",
        "meta_business_df = pd.read_csv(meta_file_path)\n",
        "meta_business_df = meta_business_df.rename(columns={\"name\": \"business_name\", \"category\": \"business_category\"})\n",
        "\n",
        "# Select only the relevant columns from meta-review-business\n",
        "meta_subset_df = meta_business_df[[\"gmap_id\", \"business_name\", \"business_category\"]]\n",
        "\n",
        "# Filter reviews for the busiest weekday\n",
        "reviews_on_busiest_day = df[df[\"workday\"] == busiest_workday]\n",
        "\n",
        "# Join with meta-business to get business names and categories\n",
        "merged_df = pd.merge(reviews_on_busiest_day, meta_subset_df, on=\"gmap_id\", how=\"left\")\n",
        "\n",
        "# Need to ensure rating values are floats\n",
        "merged_df[\"rating\"] = pd.to_numeric(merged_df[\"rating\"], errors='coerce')\n",
        "\n",
        "# Calculate average rating per business\n",
        "avg_ratings = merged_df.groupby(\"business_name\")[\"rating\"].mean()\n",
        "\n",
        "# Find business(es) with highest average rating\n",
        "max_avg_rating = avg_ratings.max()\n",
        "top_businesses = avg_ratings[avg_ratings == max_avg_rating].index.tolist()\n",
        "\n",
        "print(f\"The number of business that recived an average rating of {max_avg_rating} on {busiest_workday}'s is {len(top_businesses)}.\")\n",
        "\n",
        "# Filter merged_df to include only top-rated businesses\n",
        "filtered_df = merged_df[merged_df[\"business_name\"].isin(top_businesses)]\n",
        "\n",
        "# Select business name and category\n",
        "top_bus_cat = filtered_df[[\"business_name\", \"business_category\"]]\n",
        "\n",
        "# Drop duplicates\n",
        "top_bus_cat = top_bus_cat.drop_duplicates(\"business_name\").reset_index(drop=True)\n",
        "\n",
        "# Categories are stories as strings, convert them to a list\n",
        "top_bus_cat[\"business_category\"] = top_bus_cat[\"business_category\"].apply(ast.literal_eval)\n",
        "\n",
        "# Extract only the first category from each list\n",
        "top_bus_cat[\"primary_category\"] = top_bus_cat[\"business_category\"].apply(lambda x: x[0] if isinstance(x, list) and x else None)\n",
        "\n",
        "# Count the most common primary categories\n",
        "category_counts = top_bus_cat[\"primary_category\"].value_counts()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_counts.plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.title(\"Most Common Primary Categories Among Top-Rated Businesses\")\n",
        "plt.xlabel(\"Primary Category\")\n",
        "plt.ylabel(\"Number of Businesses\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F7TF9atyqRZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative, show top 15\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 15  # top count\n",
        "\n",
        "top = category_counts.nlargest(N).copy()\n",
        "other = category_counts.drop(top.index).sum()\n",
        "if other > 0:\n",
        "    top.loc['Other'] = other\n",
        "\n",
        "top = top.drop('Other', errors='ignore')  # Remove \"other\" category because it\n",
        "# is too large for the purpose of the visual\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "top.sort_values().plot(kind='barh')\n",
        "plt.title(f\"Top 15 Primary Categories Among Top-Rated Businesses\" )\n",
        "plt.xlabel(\"Number of Businesses\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XymGV-LdtFa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative 2\n",
        "# Wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "freq = dict(category_counts)\n",
        "wc = WordCloud(width=1000, height=500, background_color=\"white\").generate_from_frequencies(freq)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wc)\n",
        "plt.axis('off')\n",
        "plt.title(\"Category Word Cloud\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IXhyqi9otvJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.4 (use existing code)"
      ],
      "metadata": {
        "id": "OaCedhcd42dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 1.4 Find top 30 common words in reviews. Generate word clouds by review year.\n",
        "\n",
        "Decided to use pySpark for this question as we are ptentially working with large\n",
        "data.\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql.functions import explode, split, lower, col, regexp_replace, year, to_date, collect_list, flatten\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create new dataframe with only the columns we need.\n",
        "words_sdf = sdf_cleaned_with_date.select(\"newtime\", \"text\")\n",
        "\n",
        "# Create \"year\" column from \"newtime\" to date and extract year.\n",
        "words_sdf = words_sdf.withColumn(\"year\",\n",
        "                               year(to_date(col(\"newtime\"), \"yyyy-MM-dd\")))\n",
        "\n",
        "# For entries in the text column, remove punctuation and special charaters,\n",
        "# keeping only letter, numbers and spaces.\n",
        "# Convert the remaining text to lower case and split into a list of words at spaces\n",
        "# Rename the column word_list\n",
        "words_sdf = words_sdf.withColumn(\"words_list\",\n",
        "        split(\n",
        "            lower(\n",
        "                regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\s]\", \"\")\n",
        "            ), \"\\s+\"\n",
        "        )\n",
        ")\n",
        "\n",
        "# Remove stop words (“the”, “is”, “and”) words using StopWordsRemover\n",
        "# Get default stop words\n",
        "default_stopwords = StopWordsRemover().getStopWords()\n",
        "\n",
        "# In 1.1.1 we replaced NULL entries with \"no review\".\n",
        "custom_stopwords = default_stopwords + [\"no\", \"review\"]\n",
        "\n",
        "# Input column is words_list output column filtered\n",
        "remover = StopWordsRemover(inputCol=\"words_list\",\n",
        "                           outputCol=\"filtered_words\").setStopWords(custom_stopwords)\n",
        "\n",
        "# Do the removal\n",
        "words_sdf = remover.transform(words_sdf)\n",
        "\n",
        "# Group by year and collect all words into one list for each year flattening the list for each year\n",
        "grouped_yrs = words_sdf.groupBy(\"year\").agg(flatten(collect_list(\"filtered_words\")).alias(\"all_words\"))\n",
        "\n",
        "# Convert to Pandas for word cloud generation\n",
        "grouped_yrs_pd = grouped_yrs.toPandas()\n",
        "\n",
        "# Sort by year in ascending order\n",
        "grouped_yrs_pd_sorted = grouped_yrs_pd.sort_values(by=\"year\")\n",
        "\n",
        "# Count words and generate word clouds\n",
        "year_word_counts = {}\n",
        "for _, row in grouped_yrs_pd_sorted.iterrows():\n",
        "    year = row[\"year\"]\n",
        "    words = row[\"all_words\"]\n",
        "    word_freq = Counter(words).most_common(30)\n",
        "    year_word_counts[year] = dict(word_freq)\n",
        "\n",
        "    # Generate word cloud for each year\n",
        "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(year_word_counts[year])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud for {int(year)}\")\n",
        "    plt.show()\n",
        "\n",
        "# Overall word cloud\n",
        "overall_counter = Counter()\n",
        "for freq_dict in year_word_counts.values():\n",
        "    overall_counter.update(freq_dict)\n",
        "\n",
        "overall_top_30 = dict(overall_counter.most_common(30))\n",
        "\n",
        "# Plot\n",
        "wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(overall_top_30)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Overall Word Cloud\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fskh3X3DsJeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes"
      ],
      "metadata": {
        "id": "hB1bsKzT485N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backup\n",
        "\n",
        "sdf_with_newtime_column_long = sdf_text_without_null.withColumn(\n",
        "    'newtime_long',col('time').cast('long'))\n",
        "\n",
        "sdf_with_newtime_column_long.show(5)\n",
        "\n",
        "\n",
        "sdf_with_newtime_column_seconds = sdf_with_newtime_column_long.withColumn(\n",
        "    'newtime_unix_seconds', col('newtime_long') / 1000)\n",
        "\n",
        "sdf_with_newtime_column_date = sdf_with_newtime_column_seconds.withColumn(\n",
        "    'newtime', to_date(col('newtime_unix_seconds')))\n",
        "\n",
        "sdf_with_newtime_column_date.show(5)"
      ],
      "metadata": {
        "id": "dCjhbMqe5gtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP - Data wrangling - Infer schema after read Q 1.2.1\n",
        "\n",
        "# Inspecting the data for mistakes\n",
        "inspect_data = sdf_with_new_date.describe()\n",
        "inspect_data.show(truncate=False)\n",
        "\n",
        "# Define schema and parse data, nullify type mismatches.\n",
        "\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"user_id\",     T.DoubleType(),  True),\n",
        "    T.StructField(\"name\",        T.StringType(),  True),\n",
        "    T.StructField(\"time\",       T.IntegerType(), True),\n",
        "    T.StructField(\"text\",        T.StringType(),  True),\n",
        "    T.StructField(\"pics\",       T.StringType(), True),\n",
        "    T.StructField(\"resp\",       T.StringType(), True),\n",
        "    T.StructField(\"gmap_id\",     T.StringType(),  True),\n",
        "    T.StructField(\"newtime\",     T.DateType(),    True),\n",
        "])\n",
        "# user_id has the most non-null values before clean-up and is the most likely,\n",
        "# column to be a 'key', this column is required to be non-null after casting.\n",
        "\n",
        "required = ['user_id']\n",
        "\n",
        "sdf_cast = sdf_with_new_date.select(\n",
        "    [F.col(f.name).cast(f.dataType).alias(f.name) for f in schema.fields]\n",
        ")\n",
        "\n",
        "sdf_clean = sdf_cast.dropna(subset = required) # Drop nulls from userid\n",
        "\n",
        "# This is a little bit backwards, normally the data should be read-in with an\n",
        "# explicit schema from source. In this case it is being used for clean-up later\n",
        "# in the pipeline\n",
        "\n",
        "inspect_cleaned_data = sdf_clean.describe()\n",
        "\n",
        "inspect_cleaned_data.show(truncate = False)"
      ],
      "metadata": {
        "id": "RHY4ZZCOi2H9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}